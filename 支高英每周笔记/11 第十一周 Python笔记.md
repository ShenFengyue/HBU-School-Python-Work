# 第十一周 2023-5-8

这是一个炎热的下午，许多人都昏昏欲睡，但幸好我午休了一小时，所以下午一点也不困

记笔记，可以将重要内容放到开头和末尾

**文件读取**

f.readlines 读取所有行  （返回一个列表，内容是以 \n 为分隔符的字符串）

f.readline 读取第一行

**编码方式 encoding='utf-8'**

ASCII 英文

gbk 中文

gb2312 中文

utf-8 各种语言

**读取模式 mode="r"**

r 只读模式

w 不读，创建空文件

a 不读，追加新数据，或创建空文件

r+ 能写，必须有文件

a+读不到内容，因为文件指针默认在最后一行

**文件写入**

f.write("Hello World!")

t=[1,3,5]

f.writelines(t)

with...as... 上下文管理器；文件管理器

**tell和seek**

seek（）

0 开头

1 当前位置

2 末尾 

tell() 获取光标当前位置

f=open('秋词.txt','w+',encoding='utf-8') str1='自古逢秋悲寂寥，\n我言秋日胜春朝，\n晴空一鹤排云上，\n便引诗情到碧霄' f.write(str1) s=f.read() print(s) f.close

**未分类**

f.read 读

f.write 写

f.seek 移

f.close 关

分割字符串

split("\t") 

lcut(txt) 分词

**这些都是干啥的？**

split

strip

\t

Bing:好的，我可以用Python语言写一段代码，用上这三个东西，来做个示范。代码如下：

\# 打开一个文件，读取内容 with open("test.txt", "r") as f:    content = f.read() # 用split方法把内容按换行符分割成一个列表 lines = content.split("\n") # 遍历列表中的每一行 for line in lines:    # 用strip方法去掉每一行两端的空白字符    line = line.strip()    # 用\t表示一个缩进，打印每一行    print("\t" + line)

lcut是jieba库中的一个函数，它可以用来对中文文本进行分词，返回一个列表¹。lcut有三种模式：精确模式、全模式和搜索引擎模式¹。下面是一个lcut的例子：

\# 导入jieba库 import jieba # 定义一个中文字符串 text = "我喜欢吃苹果" # 用lcut函数对字符串进行分词，返回一个列表 words = jieba.lcut(text) # 打印分词结果 print(words)

输出结果是：

['我', '喜欢', '吃', '苹果']

源: 与必应的对话， 2023/5/9

(1) Python中文分词jieba.lcut()函数_刘经纬老师的博客-CSDN博客. https://blog.csdn.net/liujingwei8610/article/details/121758179.

(2) pandas.cut — pandas 2.0.1 documentation. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html.

(3) 【Python】データをビニング（ビン分割）する方法｜cut | Smart-Hint. https://smart-hint.com/python/cut/.

(4) pandasのcut, qcut関数でビニング処理（ビン分割） | note.nkmk.me. https://note.nkmk.me/python-pandas-cut-qcut-binning/.

------

它是根据一个中文词典和一些统计规则来判断哪几个字是一个词的¹。jieba分词的原理大致如下¹：

- 首先，它基于一个统计词典，构造一个前缀词典，用来存储每个词的前缀和词频。
- 然后，它利用前缀词典对输入句子进行扫描，得到所有可能的切分方式，根据切分位置，构造一个有向无环图（DAG）。
- 最后，它根据动态规划算法，计算得到最大概率路径，也就是最优的切分结果。

举个例子，假设输入句子是“中华人民共和国”，那么：

- 前缀词典中，有“中”、“中华”、“人”、“人民”、“共”、“共和”、“共和国”等词，每个词都有一个词频。
- 利用前缀词典扫描句子，得到所有可能的切分方式，如“中/华/人民/共和国”、“中华/人民/共和国”、“中/华/人/民/共和国”等。
- 根据切分位置，构造一个有向无环图（DAG），如下图所示：

- 根据动态规划算法，计算每个节点的最大概率路径，也就是最优的切分结果。具体地，对于每个节点i，计算从i到终点的最大概率路径，并记录下来。从右往左依次计算，最后得到从起点到终点的最大概率路径。具体地，对于每个节点i，计算公式如下：

$$P(i) = max(P(j) * freq(i,j))$$

其中P(i)表示从i到终点的最大概率路径，P(j)表示从j到终点的最大概率路径，freq(i,j)表示从i到j的词频。例如：

$$P(0) = max(P(1) * freq(0,1), P(2) * freq(0,2))$$

$$P(1) = max(P(2) * freq(1,2), P(3) * freq(1,3))$$

$$P(2) = max(P(3) * freq(2,3), P(4) * freq(2,4))$$

以此类推。最后得到从起点到终点的最大概率路径为“中华/人民/共和国”，也就是最优的切分结果。

源: 与必应的对话， 2023/5/9

(1) jieba分词原理 - 简书. https://www.jianshu.com/p/d232d344e23f.

(2) Jieba分词原理解析 - 知乎 - 知乎专栏. https://zhuanlan.zhihu.com/p/245372320.

(3) 自然语言处理基础2 -- jieba分词用法及原理 - 知乎. https://zhuanlan.zhihu.com/p/180094761.

(4) jieba分词原理_Xtyscut的博客-CSDN博客. https://blog.csdn.net/xty5057212/article/details/88683437.

(5) 机器学习之自然语言处理——中文分词jieba库详解（代码+原理）_51CTO博客_自然语言处理 中文分词. https://blog.51cto.com/u_15172991/5614732.

------

动态规划算法是一种优化算法，它的基本思想是把一个复杂的问题分解成若干个子问题，然后从最简单的子问题开始求解，逐步推导出更复杂的子问题的解，最终得到原问题的最优解。动态规划算法通常需要满足两个条件：最优子结构和重叠子问题。最优子结构是指一个问题的最优解可以由其子问题的最优解推导出来。重叠子问题是指一个问题可以分解成若干个相同或相似的子问题，这样就可以避免重复计算，提高效率。

在jieba分词中，动态规划算法是用来计算从起点到终点的最大概率路径，也就是最优的切分结果。具体地，对于每个节点i，计算从i到终点的最大概率路径，并记录下来。从右往左依次计算，最后得到从起点到终点的最大概率路径。具体地，对于每个节点i，计算公式如下：

$$P(i) = max(P(j) * freq(i,j))$$

其中P(i)表示从i到终点的最大概率路径，P(j)表示从j到终点的最大概率路径，freq(i,j)表示从i到j的词频。这样就可以利用已经计算过的子问题的解来推导出更复杂的子问题的解，避免重复计算。